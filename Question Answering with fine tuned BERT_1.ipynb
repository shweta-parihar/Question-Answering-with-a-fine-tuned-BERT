{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question Answering with fine tuned BERT_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNiet6Ktu5kOuLQk8tpk+uj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-L9a1w_OuiFN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnUfeFEfs2ef"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMSAseF2pZk4","executionInfo":{"status":"ok","timestamp":1638893232474,"user_tz":-330,"elapsed":8639,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"c056bf8d-9cdd-44df-d9d4-ba6b73646323"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 66.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 501 kB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 59.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 19.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"afeulp7KpbYj"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["64ddb1a7b4fe4fbe8d101a17e87beb0d","c0c88b81e1c6455ea75f20bb6f9a7d74","2497b83e75044985acca1bd0799c1382","1c0b8ea76e4e4b928463cb497ec226d4","a8bd475f42d04de881e3a73b68a1cd5a","d198c5a619ee4091b217597af36b46e9","4446f74381c14ae4bdbbbf65820e4302","817c4986ded64cf097182e05fbdc4ce8","fa9173e66be84c9e9d903a7527aa8472","553a9c8b739a45308c398faf8baafefd","e4c658b49b3c431080c3e39051f425c6","f3047a6de7fc4800a4ca462b1533f2f8","287510b343c84b3a9007397483304bd5","d5bc62850c6e427286ce079fcd904290","0cd74767cb3649b8b46d6d698c7d0e59","0567ffb97b744d0d9d721664954eaeb5","bf90e2dc32c94e0897b139bc3eb0186c","48a7265527e3462492d205041add501b","fd22f12a6bfc40cba39467bd7a7e5646","80802ef40b3747f881ec711273cf9a4e","c97498e2b52a48fb9f57bdad49d211f9","343ddc19b50f400b8f1457692e4f13ba"]},"id":"xJJJuHbRpdWR","executionInfo":{"status":"ok","timestamp":1638893280223,"user_tz":-330,"elapsed":42345,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"25e3c61d-a9e9-43f4-ebb2-80b4a90f41fd"},"source":["from transformers import BertForQuestionAnswering\n","\n","model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64ddb1a7b4fe4fbe8d101a17e87beb0d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3047a6de7fc4800a4ca462b1533f2f8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["1f2ac75f1d404b3ebcb0eccce53b5cbc","f1e4a0e472f5416c856ee890dd70797b","bdae9325bf4a4430b2bcb5a8b7c943c6","725089ec2d2e4980a0912dba889896d2","dddecb367a484fab895d4a715bd08c43","6ec3b33b85174a629611f147d3c3bc62","b9285aa800ff4ff69874a970835c1e5e","f399653071914c7083a3d6d646883169","8fe6e454e74e41cf925f2840faa839f4","8a7e848f7ad446a8a8dad5662a51b6c4","6b47869a62ea4563b320a78bc9ae1266","66cbc180039f4acc942f1a86d2c5f1a5","b9f9ba724e01426686ca74eb05b6dd18","4407502a1e72444dac31b30262543258","fae87b15cfd74f9eb2b516a9b754272b","d67a8e64f59b42debb0c72a0f93d6b02","2ddd41d758b743db9d16b997ac17d341","4cc509bf7d6c44a0a0c2833449e8c1af","60e385a375d24b18b49fdd48a1b6d732","6f1195b2ba0c4d139dbb40fe6137f2fe","8f324000d78e456aac73bb3fffafe4b3","350b50e47243409a87770ac49006f29b","41f1b632883a4914bcf62470646b5540","57bdd144807448cc80c8f4c7b7a6b507","75653936e352474b8222008416347b9d","fe7a265a02b744499d88b92d903f3114","32c4185c84214d5ea85cee7c400e9fae","5867863d3bb1468fbcd22a502f665833","f8e493af1c054a99833fbdf140d38d12","98143cc3bc7140ec88e22c1b55097594","4d9449077eac4b648507208f7212d740","1f4687cfb531439c91c034380bd4df60","5f3011bcd4f44b9ba18910313177cc5a"]},"id":"TemV1bAwrDD0","executionInfo":{"status":"ok","timestamp":1638893642119,"user_tz":-330,"elapsed":10186,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"f88fdde0-d48c-40aa-bff5-c532c8d8e877"},"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f2ac75f1d404b3ebcb0eccce53b5cbc","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66cbc180039f4acc942f1a86d2c5f1a5","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41f1b632883a4914bcf62470646b5540","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"s7lasT2RphjX"},"source":["question = \"How many parameters does BERT-large have?\"\n","answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZJZeCBspqow","executionInfo":{"status":"ok","timestamp":1638893642122,"user_tz":-330,"elapsed":35,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"b5c87389-7c84-4265-e85b-df412e690b20"},"source":["# Apply the tokenizer to the input text, treating them as a text-pair.\n","input_ids = tokenizer.encode(question, answer_text)\n","\n","print('The input has a total of {:} tokens.'.format(len(input_ids)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The input has a total of 70 tokens.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwakpcM1qxGO","executionInfo":{"status":"ok","timestamp":1638894331267,"user_tz":-330,"elapsed":583,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"f54f185e-6b5e-4808-9e08-7d62f891183d"},"source":["# BERT only needs the token IDs, but for the purpose of inspecting the tokenizer's behavior, let's also get the token strings and display them.\n","tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","# For each token and its id...\n","for token, id in zip(tokens, input_ids):\n","    \n","    # If this is the [SEP] token, add some space around it to make it stand out.\n","    if id == tokenizer.sep_token_id:\n","        print('')\n","    \n","    # Print the token string and its ID in two columns.\n","    print('{} --- {}'.format(token, id))\n","\n","    if id == tokenizer.sep_token_id:\n","        print('')\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] --- 101\n","how --- 2129\n","many --- 2116\n","parameters --- 11709\n","does --- 2515\n","bert --- 14324\n","- --- 1011\n","large --- 2312\n","have --- 2031\n","? --- 1029\n","\n","[SEP] --- 102\n","\n","bert --- 14324\n","- --- 1011\n","large --- 2312\n","is --- 2003\n","really --- 2428\n","big --- 2502\n",". --- 1012\n",". --- 1012\n",". --- 1012\n","it --- 2009\n","has --- 2038\n","24 --- 2484\n","- --- 1011\n","layers --- 9014\n","and --- 1998\n","an --- 2019\n","em --- 7861\n","##bed --- 8270\n","##ding --- 4667\n","size --- 2946\n","of --- 1997\n","1 --- 1015\n",", --- 1010\n","02 --- 6185\n","##4 --- 2549\n",", --- 1010\n","for --- 2005\n","a --- 1037\n","total --- 2561\n","of --- 1997\n","340 --- 16029\n","##m --- 2213\n","parameters --- 11709\n","! --- 999\n","altogether --- 10462\n","it --- 2009\n","is --- 2003\n","1 --- 1015\n",". --- 1012\n","34 --- 4090\n","##gb --- 18259\n",", --- 1010\n","so --- 2061\n","expect --- 5987\n","it --- 2009\n","to --- 2000\n","take --- 2202\n","a --- 1037\n","couple --- 3232\n","minutes --- 2781\n","to --- 2000\n","download --- 8816\n","to --- 2000\n","your --- 2115\n","cola --- 15270\n","##b --- 2497\n","instance --- 6013\n",". --- 1012\n","\n","[SEP] --- 102\n","\n"]}]},{"cell_type":"code","metadata":{"id":"vIP5lUA0rJJD"},"source":["# Search the input_ids for the first instance of the `[SEP]` token.\n","sep_index = input_ids.index(tokenizer.sep_token_id)\n","\n","# The number of segment A tokens includes the [SEP] token istelf.\n","token_type_a = sep_index + 1\n","\n","# The remainder are segment B.\n","token_type_b = len(input_ids) - token_type_a\n","\n","# Construct the list of 0s and 1s.\n","token_type_ids = [0]*token_type_a + [1]*token_type_b\n","\n","# There should be a segment_id for every input token.\n","assert len(token_type_ids) == len(input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XSuRP8fZrwJz"},"source":["# Run our example through the model.\n","outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n","                             token_type_ids=torch.tensor([token_type_ids]), # The token type ids to differentiate question from answer_text\n","                             return_dict=True) \n","\n","start_scores = outputs.start_logits\n","end_scores = outputs.end_logits\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUgjvJsdr1YP","executionInfo":{"status":"ok","timestamp":1638893841669,"user_tz":-330,"elapsed":673,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"3cbade67-a914-496e-f2a6-2ee2b563d94e"},"source":["# Find the tokens with the highest `start` and `end` scores.\n","answer_start = torch.argmax(start_scores)\n","answer_end = torch.argmax(end_scores)\n","\n","# Combine the tokens in the answer and print it out.\n","answer = ' '.join(tokens[answer_start:answer_end+1])\n","\n","print('Answer: \"' + answer + '\"')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer: \"340 ##m\"\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxWvYyiZsAxb","executionInfo":{"status":"ok","timestamp":1638893886167,"user_tz":-330,"elapsed":496,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"ad404bce-d3c9-4e13-cb64-57f72f102c5b"},"source":["# Start with the first token.\n","answer = tokens[answer_start]\n","\n","# Select the remaining answer tokens and join them with whitespace.\n","for i in range(answer_start + 1, answer_end + 1):\n","    \n","    # If it's a subword token, then recombine it with the previous token.\n","    if tokens[i][0:2] == '##':\n","        answer += tokens[i][2:]\n","    \n","    # Otherwise, add a space then the token.\n","    else:\n","        answer += ' ' + tokens[i]\n","\n","print('Answer: \"' + answer + '\"')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer: \"340m\"\n"]}]},{"cell_type":"code","metadata":{"id":"tRyOM-2VtD2K"},"source":["def answer_question(question, answer_text):\n","    '''\n","    Takes a `question` string and an `answer_text` string (which contains the\n","    answer), and identifies the words within the `answer_text` that are the\n","    answer. Prints them out.\n","    '''\n","    # ======== Tokenize ========\n","    # Apply the tokenizer to the input text, treating them as a text-pair.\n","    input_ids = tokenizer.encode(question, answer_text)\n","\n","    # Report how long the input sequence is.\n","    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n","\n","    # ======== Set Segment IDs ========\n","    # Search the input_ids for the first instance of the `[SEP]` token.\n","    sep_index = input_ids.index(tokenizer.sep_token_id)\n","\n","    # The number of segment A tokens includes the [SEP] token istelf.\n","    num_seg_a = sep_index + 1\n","\n","    # The remainder are segment B.\n","    num_seg_b = len(input_ids) - num_seg_a\n","\n","    # Construct the list of 0s and 1s.\n","    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","\n","    # There should be a segment_id for every input token.\n","    assert len(segment_ids) == len(input_ids)\n","\n","    # ======== Evaluate ========\n","    # Run our example through the model.\n","    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n","                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n","                    return_dict=True) \n","\n","    start_scores = outputs.start_logits\n","    end_scores = outputs.end_logits\n","\n","    # ======== Reconstruct Answer ========\n","    # Find the tokens with the highest `start` and `end` scores.\n","    answer_start = torch.argmax(start_scores)\n","    answer_end = torch.argmax(end_scores)\n","\n","    # Get the string versions of the input tokens.\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","    # Start with the first token.\n","    answer = tokens[answer_start]\n","\n","    # Select the remaining answer tokens and join them with whitespace.\n","    for i in range(answer_start + 1, answer_end + 1):\n","        \n","        # If it's a subword token, then recombine it with the previous token.\n","        if tokens[i][0:2] == '##':\n","            answer += tokens[i][2:]\n","        \n","        # Otherwise, add a space then the token.\n","        else:\n","            answer += ' ' + tokens[i]\n","\n","    print('Answer: \"' + answer + '\"')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmrGQj2auFPu","executionInfo":{"status":"ok","timestamp":1638894478496,"user_tz":-330,"elapsed":3744,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"23966ed6-3a23-4d4d-be87-18de41a24428"},"source":["text_passage = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n","question = \"What does the 'B' in BERT stand for?\"\n","\n","answer_question(question, text_passage)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query has 258 tokens.\n","\n","Answer: \"bidirectional encoder representations from transformers\"\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8B5BtY6uT68","executionInfo":{"status":"ok","timestamp":1638894495993,"user_tz":-330,"elapsed":3765,"user":{"displayName":"Shweta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq5_h50AUuKg6-B711cwbRxwkwycz5EMbKCcqyiA=s64","userId":"05242245977803223502"}},"outputId":"0737aa6e-25d6-4cfe-9cea-d428617caf33"},"source":["question = \"What are some example applications of BERT?\"\n","\n","answer_question(question, text_passage)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query has 255 tokens.\n","\n","Answer: \"question answering and language inference\"\n"]}]}]}